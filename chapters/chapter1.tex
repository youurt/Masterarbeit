\chapter{Einleitung}\label{ch1}

\section{Motivation}

Das Aufkommen von Online-Nachrichtenagenturen und die Explosion der Anzahl der Benutzer, die Nachrichten mit diesem Medium konsumieren, haben dazu geführt, dass mehrere Webseiten miteinander konkurrieren, um die Aufmerksamkeit der Benutzer zu erregen. Dies hat dazu geführt, dass die Medien kreative Wege geschaffen haben, um Leser auf ihre Website zu locken. Eine der am häufigsten verwendeten Techniken ist die Verwendung von Clickbait-Überschriften. Diese Überschriften wurden speziell dafür entwickelt, um das Interesse des Lesers an dem zu wecken, was versprochen wird. Wenn auf den Artikel geklickt wird jedoch, liefert dieser Artikel normalerweise nicht den Inhalt, den der Leser ursprünglich gesucht hat. 


Deep Learning gewinnt immer mehr Beliebtheit, neben der Wissenschaft, kann Deep Learning auch kommerziell für den Endbenutzer entwickelt werden. Durch \textit{TensorFlow.js}, einer für JavaScript entwickelten Version von der beliebten Deep Learning Bibliothek TensorFlow, ist es möglich Deep Learning Anwendungen \enquote{sehr nah} am Endbenutzer zu entwickeln und somit ohne einen Server Inferenz zu bilden. Diese ist eine relativ neue Art und Weise, Deep Learning Anwendungen zu schreiben. Sie wird in dieser Arbeit als \enquote{clientseitiges Deep Learning} beschrieben. Clientseitiges Deep Learning macht das Deep Learning flexibler, denn es kann im Browser auf dem Server oder auf einem mobilen Endgerät laufen. Es kann schneller sein, da es in sehr kurzer Zeit eine Antwort geben kann, da die Kommunikation mit einem Server entfällt. Der Nutzer muss außerdem seine Daten nicht preisgeben.

Für deutsche Clickbaits gibt es keinen Datensatz. Um eine binäre Klassifikation durchzuführen wird ein Datensatz erstellt. Dieser Datensatz kann in seinem Rohdatenformat falsche Aussagen treffen. Da die Seiten woher die Daten stammen nicht reine Clickbaits anbieten, sondern auch Nachrichten die nicht in diese Kategorie fallen. Für das trennen dieser Daten ist ein händisches Labeln notwendig, was Zeit und Ressourcen in Anspruch nimmt. Dieses Problem kann durch \enquote{automatisches Labeln} behoben werden. Dazu wird es einmal Nötig sein, eine Literaturanalyse zu machen, um zu sehen, wie Clickbaits im Vorfeld automatisch erkannt und getrennt werden können.


\section{Forschungsfragen}
Die Arbeit soll folgende Fragen beantworten:
\begin{enumerate}
    \item Wie kann ein Datensatz für deutsche Clickbaits erstellt werden?
    \item Wie gut können aus den ermittelten Daten ein Modell trainiert werden?
    \item Wie können Deep Learning Modelle in eine Web-Anwendung eingebettet werden?
\end{enumerate}
Diese Fragen werden im Kapitel \ref{ch9} beantwortet.

\section{Aufbau der Arbeit}

Die Arbeit beginnt im \textbf{Kapitel~\ref{ch2}} mit einem theoretischen Einstieg über Deep Learning. Zunächst werden Grundlegende Themen behandelt, die dem Leser einen Einstieg in die praktischen Anwendungen, welche in den späteren Kapiteln erfolgen, näher erläutern sollen. Um den Scope der Arbeit nicht ins unendliche zu ziehen, werden die Algorithmen \textit{Gradientenabstiegsverfahren} und \textit{Backpropagation} nicht sehr tief behandelt. Es werden nur wichtige und relevante Themen hineingezogen. Da später Experimente mit dem Modell gemacht werden, sind die \enquote{Hyperparameter} bei Deep Learning Modellen bedeutsam. Diese Parameter sind z.B. die \textit{Lernrate} und das Konzept der \textit{Über-} und \textit{Unteranpassung}. Es gilt das Modell soweit es geht zu optimieren, mit bestimmten Maßnahmen wie \textit{Regularisierung}. In diesem Kapitel soll auch gezeigt werden, wie Deep Learning Modelle lernen. Es stellt sich heraus, dass dafür bestimmte Konzepte wie \textit{Aktivierungsfunktionen} oder \textit{Verlustfunktionen} vorgestellt werden müssen. Es gibt immer mehrere Möglichkeiten ein Problem mittels Deep Learning zu lösen. Hier wird der Fokus auf ein Netzwerk, der \textit{CNN}-Netzwerke vorgestellt. Das CNN-Netzwerk wird in den späteren Kapiteln angewendet, jedoch bedarf es an theoretischer Grundlage.


Im \textbf{Kapitel~\ref{ch3}} geht es um die Verarbeitung der natürlichen Sprache. Es werden zunächst die standardisierten Verfahren wie \textit{Reguläre Ausdrücke} vorgestellt. Diese können dafür verwendet werden, um ein Text in seine einzelnen Elemente \textit{Tokens} umzuwandeln, da die Tokenisierung ein wichtiger und häufig angewendeter Prozess ist. Neben dem Verfahren der Tokenisierung, gibt es andere Verfahren im \textit{NLP}. Das \textit{Tagging} der Wörter mit der zugehörigen Grammatik wird auch in diesem Kapitel behandelt. Eine Frage die beantwortet werden muss ist, wie Computer die natürliche Sprache lernen. Hier wird zunächst dargestellt, dass ein Computer nur Zahlen versteht und eine Umwandlung von Wörtern in Zahlen gelingen muss. Sicherlich gibt es dafür mehrere Verfahren, hier wird die Methode der \textit{Worteinbettungen} vorgestellt. Kapitel~\ref{ch3} wird damit abgeschlossen, indem eine Verbindung mit Kapitel~\ref{ch2} hergestellt wird. CNN werden häufig für Bilddateien verwendet. Hier wird gezeigt, wie CNNs auch bei der Textverarbeitung angewendet werden können.

Gemeinsam mit Kapitel~\ref{ch2} und Kapitel~\ref{ch3} wird der theoretische Teil der Arbeit abgeschlossen und im \textbf{Kapitel~\ref{ch4}} geht es dann um die Literatur. Um Clickbaits zu klassifizieren ist eine Definition von Clickbaits nötig. Dieses ist eine Herausforderung, denn Clickbaits sind sehr subjektiv und können nur sehr schwer definiert werden. Es gibt jedoch einige \enquote{Muster} die aus mehreren Arbeiten zu entnehmen sind, die sich mit diesem Thema beschäftigt haben. Es soll in diesem Kapitel auch erforscht werden, welche Ansätze es gibt, in Bezug auf \textit{Klassifizierung von Clickbaits}. Hier wird gezeigt, wie verwandte Arbeiten das Problem gelöst. Diese Arbeiten werden in den nächsten Kapiteln als Grundlage für weiteres Vorgehen verwendet werden können.



Im \textbf{Kapitel~\ref{ch5}} wird das Konzept der Arbeit gemeinsam mit der angewendeten Methodik vorgestellt. Dieser Teil der Arbeit soll den praktischen Teil der Arbeit begründen. Es gibt viele Möglichkeiten wie ein Problem gelöst werden kann, hier wird erläutert, was die Gründe waren. Es wird quasi eine \enquote{Machbarkeitsanalyse} vollzogen. Dieses Kapitel soll die nachfolgenden \textbf{Kapiteln \ref{ch 6}, Kapitel \ref{ch7} und \ref{ch8}} einleiten.  Im Kapitel \ref{ch 6} geht es zuerst darum, wie der Datensatz generiert wurde. Im Kapitel \ref{ch7} wird das Deep Learning Modell entwickelt, aus den Daten die im Kapitel \ref{ch 6} erstellt wurden. Hier findet neben dem Training auch die Evaluation des Modells statt. Im Kapitel \ref{ch8} befindet sich der Ansatz, das ganze Modell in eine Webanwendung einzubetten. Im \textbf{Kapitel \ref{ch9}} wird schließlich die Arbeit abgeschlossen, indem die Forschungsfragen beantwortet und ein Ausblick über Verbesserungsmöglichkeiten der Arbeit gegeben werden.